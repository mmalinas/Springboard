{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>Onion?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Louisiana eye doctor offers free eye exams for...</td>\n",
       "      <td>114318</td>\n",
       "      <td>aic7pm</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>https://www.wwltv.com/article/sports/nfl/saint...</td>\n",
       "      <td>2102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.548120e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man rescued from Taliban didn't believe Donald...</td>\n",
       "      <td>103947</td>\n",
       "      <td>76rjtv</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>http://www.newsweek.com/man-rescued-taliban-di...</td>\n",
       "      <td>5288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.508199e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nat Geo hires Jeff Goldblum to walk around, be...</td>\n",
       "      <td>100839</td>\n",
       "      <td>923ww4</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>https://news.avclub.com/nat-geo-hires-jeff-gol...</td>\n",
       "      <td>1537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.532652e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black security guard who stops shooter is then...</td>\n",
       "      <td>100022</td>\n",
       "      <td>9wl2d7</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>https://thehill.com/homenews/news/416255-black...</td>\n",
       "      <td>2399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.542106e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hunter dies after shot elephant falls on him</td>\n",
       "      <td>95389</td>\n",
       "      <td>6cgr3h</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>http://www.news24.com/SouthAfrica/News/hunter-...</td>\n",
       "      <td>1712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.495405e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   score      id  \\\n",
       "0  Louisiana eye doctor offers free eye exams for...  114318  aic7pm   \n",
       "1  Man rescued from Taliban didn't believe Donald...  103947  76rjtv   \n",
       "2  Nat Geo hires Jeff Goldblum to walk around, be...  100839  923ww4   \n",
       "3  Black security guard who stops shooter is then...  100022  9wl2d7   \n",
       "4       Hunter dies after shot elephant falls on him   95389  6cgr3h   \n",
       "\n",
       "     subreddit                                                url  \\\n",
       "0  nottheonion  https://www.wwltv.com/article/sports/nfl/saint...   \n",
       "1  nottheonion  http://www.newsweek.com/man-rescued-taliban-di...   \n",
       "2  nottheonion  https://news.avclub.com/nat-geo-hires-jeff-gol...   \n",
       "3  nottheonion  https://thehill.com/homenews/news/416255-black...   \n",
       "4  nottheonion  http://www.news24.com/SouthAfrica/News/hunter-...   \n",
       "\n",
       "   num_comments body       created  Onion?  \n",
       "0          2102  NaN  1.548120e+09       0  \n",
       "1          5288  NaN  1.508199e+09       0  \n",
       "2          1537  NaN  1.532652e+09       0  \n",
       "3          2399  NaN  1.542106e+09       0  \n",
       "4          1712  NaN  1.495405e+09       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Reddit posts as csv\n",
    "import pandas as pd\n",
    "\n",
    "all_posts = pd.read_csv('all_posts_reddit_onionandnotonion_2.csv', index_col=0, sep='|')\n",
    "all_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "      <th>Onion?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>34996</td>\n",
       "      <td>7b0y34</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>https://www.theonion.com/no-way-to-prevent-thi...</td>\n",
       "      <td>3043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509951e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump Warns Removing Confederate Statues Could...</td>\n",
       "      <td>27425</td>\n",
       "      <td>7to2ak</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>https://politics.theonion.com/trump-warns-remo...</td>\n",
       "      <td>1882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.517211e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‘No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>21138</td>\n",
       "      <td>7xl7h3</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>https://www.theonion.com/no-way-to-prevent-thi...</td>\n",
       "      <td>1200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.518670e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roy Moore Retires From Politics To Spend More ...</td>\n",
       "      <td>18073</td>\n",
       "      <td>7jgh6i</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>https://politics.theonion.com/roy-moore-retire...</td>\n",
       "      <td>126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.513165e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Pence Disappointed In The 200,000 Husband...</td>\n",
       "      <td>16752</td>\n",
       "      <td>5pbaag</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>http://www.theonion.com/article/mike-pence-dis...</td>\n",
       "      <td>332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.485044e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id subreddit  \\\n",
       "0  'No Way To Prevent This,’ Says Only Nation Whe...  34996  7b0y34  TheOnion   \n",
       "1  Trump Warns Removing Confederate Statues Could...  27425  7to2ak  TheOnion   \n",
       "2  ‘No Way To Prevent This,’ Says Only Nation Whe...  21138  7xl7h3  TheOnion   \n",
       "3  Roy Moore Retires From Politics To Spend More ...  18073  7jgh6i  TheOnion   \n",
       "4  Mike Pence Disappointed In The 200,000 Husband...  16752  5pbaag  TheOnion   \n",
       "\n",
       "                                                 url  num_comments  body  \\\n",
       "0  https://www.theonion.com/no-way-to-prevent-thi...          3043   NaN   \n",
       "1  https://politics.theonion.com/trump-warns-remo...          1882   NaN   \n",
       "2  https://www.theonion.com/no-way-to-prevent-thi...          1200   NaN   \n",
       "3  https://politics.theonion.com/roy-moore-retire...           126   NaN   \n",
       "4  http://www.theonion.com/article/mike-pence-dis...           332   NaN   \n",
       "\n",
       "        created  Onion?  \n",
       "0  1.509951e+09       1  \n",
       "1  1.517211e+09       1  \n",
       "2  1.518670e+09       1  \n",
       "3  1.513165e+09       1  \n",
       "4  1.485044e+09       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts_news = pd.read_csv('all_posts_reddit_onionandnews_csv', index_col=0, sep='|')\n",
    "all_posts_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_ner(title):\n",
    "    \"\"\"function to apply English library to title\"\"\"\n",
    "    spacy_title = nlp(title)\n",
    "    return spacy_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace(string, substitutions):\n",
    "    \"\"\"Function that uses regex to substitute something in a string\"\"\"\n",
    "    substrings = sorted(substitutions, key=len, reverse=True)\n",
    "    regex = re.compile('|'.join(map(re.escape, substrings)))\n",
    "    return regex.sub(lambda match: substitutions[match.group(0)], string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(title):\n",
    "    \"\"\"Make lists of entities and entity types and return them\"\"\"\n",
    "    spacy_title = spacy_ner(title)\n",
    "    spacy_title_ents = [str(X) for X in spacy_title.ents]\n",
    "    spacy_title_ents_types = [X.label_ for X in spacy_title.ents]\n",
    "    return spacy_title_ents, spacy_title_ents_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entities(title):\n",
    "    \"\"\"Substitutes entities with empty strings and return title with no entities\"\"\"\n",
    "    entities, ent_types = get_entities(title)\n",
    "    if entities == []:\n",
    "        return title\n",
    "    else:\n",
    "        substitutions = {}\n",
    "        for X in entities:\n",
    "            substitutions[X] = ''\n",
    "        output = replace(title, substitutions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/melaniemalinas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melaniemalinas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import string\n",
    "\n",
    "def preprocessing(title):\n",
    "    \"\"\"Function to preprocess title\"\"\"\n",
    "    title_noentities = remove_entities(title) #remove entities\n",
    "    title_lower = title_noentities.lower() #make lowercase\n",
    "    title_lower_nonumbers = re.sub(r'\\d+','', str(title_lower)) #remove numbers\n",
    "    no_punctuation = re.sub(r'[^\\w\\s]','', title_lower_nonumbers) #remove punctuation\n",
    "    tokenized_title = word_tokenize(no_punctuation) #tokenize title\n",
    "    new_title = []\n",
    "    for word in tokenized_title:\n",
    "        new_word = lemmatizer.lemmatize(word) #lemmatize each word\n",
    "        new_title.append(new_word) #make title into list of words\n",
    "    final_title = [i for i in new_title if not i in stop_words] #get rid of stopwords\n",
    "    return final_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_entities(title):\n",
    "    \"\"\"Preprocess title and append entities and entity types\"\"\"\n",
    "    title_entities, title_ents_types = get_entities(title)\n",
    "    title_noentities = preprocessing(title)\n",
    "    title_all = title_noentities + title_entities + title_ents_types\n",
    "    return title_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply preprocessing function to all titles\n",
    "all_titles_ents = all_posts['title'].apply(lambda x: preprocessing_entities(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframe with only title and whether or not it is the Onion\n",
    "final_posts_df = pd.DataFrame({'title':all_titles_ents, 'Onion?':all_posts['Onion?']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making X and y arrays\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    \"\"\"just a dummy function for the CountVectorizer\"\"\"\n",
    "    return doc\n",
    "\n",
    "def make_xy(df, vectorizer=None):\n",
    "    \"\"\"make titles and Onion status into X and y arrays for machine learning\"\"\"    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(tokenizer=dummy_fun,\n",
    "        preprocessor=dummy_fun)\n",
    "    X = vectorizer.fit_transform(df['title'])\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df['Onion?'].values.astype(np.int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    \"\"\"get CV score from a classifier input\"\"\"\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(nfold, random_state=42).split(X): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit the classifier, passed is as clf.\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "#make a scorer from the F1 score\n",
    "f1_scorer = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#the grid of parameters to search over\n",
    "alphas = [.1, 1, 5, 10, 50]\n",
    "best_min_df = 5\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "maxscore=-np.inf\n",
    "for alpha in alphas:   \n",
    "    vectorizer = CountVectorizer(min_df=best_min_df, tokenizer=dummy_fun, \\\n",
    "    preprocessor=dummy_fun)\n",
    "    Xthis, ythis = make_xy(final_posts_df, vectorizer)\n",
    "    X_train_this, X_test_this, y_train_this, y_test_this = \\\n",
    "    train_test_split(Xthis, ythis, test_size=0.2, random_state=42)\n",
    "    clf = MultinomialNB(alpha=alpha).fit(X_train_this, y_train_this)\n",
    "    score = cv_score(clf,X_train_this,y_train_this, f1_scorer)\n",
    "    if score > maxscore:\n",
    "        maxscore = score\n",
    "        best_alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"alpha: {}\".format(best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.845223\n",
      "Accuracy on test data:     0.745547\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df = best_min_df, tokenizer=dummy_fun, preprocessor=dummy_fun)\n",
    "X_2, y_2 = make_xy(final_posts_df, vectorizer)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = \\\n",
    "    train_test_split(X_2, y_2, test_size=0.2, random_state=42)                                                   \n",
    "clf = MultinomialNB(alpha=best_alpha).fit(X_train_2, y_train_2)\n",
    "#Print the accuracy on the test and training dataset\n",
    "training_accuracy = clf.score(X_train_2, y_train_2)\n",
    "test_accuracy = clf.score(X_test_2, y_test_2)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.81      0.76       200\n",
      "           1       0.78      0.68      0.72       193\n",
      "\n",
      "    accuracy                           0.75       393\n",
      "   macro avg       0.75      0.74      0.74       393\n",
      "weighted avg       0.75      0.75      0.74       393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred = clf.predict(X_test_2)\n",
    "classification_report = metrics.classification_report(y_test_2,pred)\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onion words\t     P(Onion | word)\n",
      "               trump 0.96\n",
      "           regularly 0.95\n",
      "             happens 0.94\n",
      "           announced 0.93\n",
      "           announces 0.93\n",
      "                shit 0.93\n",
      "              nation 0.92\n",
      "            evidence 0.92\n",
      "            anything 0.92\n",
      "            everyone 0.92\n",
      "Not Onion words\t     P(Not Onion | word)\n",
      "                help 0.12\n",
      "             officer 0.11\n",
      "                 LOC 0.11\n",
      "             protest 0.10\n",
      "             accused 0.09\n",
      "               China 0.08\n",
      "             ORDINAL 0.06\n",
      "               Texas 0.06\n",
      "            arrested 0.06\n",
      "              police 0.05\n"
     ]
    }
   ],
   "source": [
    "words = np.array(vectorizer.get_feature_names()) #get list of words\n",
    "\n",
    "#make identity matrix so that each row has only one word\n",
    "x = np.eye(X_test_2.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:10]]\n",
    "bad_words = words[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print(\"Onion words\\t     P(Onion | word)\")\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Not Onion words\\t     P(Not Onion | word)\")\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actually not the Onion but mis-classified as the Onion\n",
      "---------------------------\n",
      "‘Live pee or die’: N.H. governor steps in to let woman keep her ‘PB4WEGO’ license plates\n",
      "\n",
      "White supremacists taking DNA tests sad to discover they’re not 100% white\n",
      "\n",
      "DeVos backlash Sees Parents Threatening to Homeschool Kids\n",
      "\n",
      "Bush says Trump ‘makes me look pretty good’ by comparison: report\n",
      "\n",
      "Rapper Soulja Boy Releases New Handheld Game Console and It Looks Terrible\n",
      "\n",
      "Actually the Onion but mis-classified as not the Onion\n",
      "--------------------------\n",
      "Blog: If You’re Not A Police Officer, You Can’t Understand The Pressure You Feel In The Split Second When You Have To Decide Whether Or Not To Shoot An Unarmed Civilian 8 Times\n",
      "\n",
      "Police Repeatedly Shoot Tim Cook After Mistaking iPhone For Gun\n",
      "\n",
      "Dallas Cops Plant Black Suspect At Murder Scene\n",
      "\n",
      "Trump Confident U.S. Military Strike On Syria Wiped Out Russian Scandal\n",
      "\n",
      "Heartbroken Russian Ambassador Thought Special Meetings With Jeff Sessions Were Very Memorable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = make_xy(final_posts_df, vectorizer)\n",
    "\n",
    "prob = clf.predict_proba(x)[:, 0] #probability of being not the onion\n",
    "predict = clf.predict(x)\n",
    "\n",
    "y = np.asarray(y)\n",
    "misclassified = np.where(y != predict) #identify where values don't match prediction\n",
    "\n",
    "series_misclassified = pd.Series(prob[misclassified], index=list(misclassified))\n",
    "\n",
    "#sort series\n",
    "\n",
    "series_misclassified_sorted = series_misclassified.sort_values() #get sorted values of misclassified\n",
    "\n",
    "indices_misclassified = list(series_misclassified_sorted.index.values) #get indices of misclassified and sort\n",
    "\n",
    "#get first and last indices\n",
    "lowest_prob = indices_misclassified[0:5]\n",
    "highest_prob = indices_misclassified[-5:]\n",
    "\n",
    "#make lists of lowest probabilities and highest probabilities\n",
    "lowest_prob_list = [item for t in lowest_prob for item in t] \n",
    "highest_prob_list = [item for t in highest_prob for item in t]\n",
    "\n",
    "###\n",
    "print(\"Actually not the Onion but mis-classified as the Onion\")\n",
    "print('---------------------------')\n",
    "for row in lowest_prob_list:\n",
    "    print(all_posts.title.iloc[row]) #index into those rows in all_posts\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Actually the Onion but mis-classified as not the Onion\")\n",
    "print('--------------------------')\n",
    "for row in highest_prob_list:\n",
    "    print(all_posts.title.iloc[row]) #index into those rows in all_posts\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to try CountVectorizer with bi-grams by editing ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_xy_bigrams(df, vectorizer=None):\n",
    "    \"\"\"Same as make_xy but with ngram_range\"\"\"    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(tokenizer=dummy_fun,\n",
    "        preprocessor=dummy_fun, ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(df['title'])\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = df['Onion?'].values.astype(np.int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the grid of parameters to search over\n",
    "alphas_2 = [.1, 1, 5, 10, 50]\n",
    "best_min_df_2 = 5\n",
    "\n",
    "#Find the best value for alpha and the best classifier\n",
    "best_alpha_2 = None\n",
    "maxscore_2=-np.inf\n",
    "\n",
    "for alpha in alphas_2:   \n",
    "    vectorizer = CountVectorizer(min_df=best_min_df_2, tokenizer=dummy_fun, \\\n",
    "    preprocessor=dummy_fun, ngram_range=(1,2))\n",
    "    Xthis, ythis = make_xy_bigrams(final_posts_df, vectorizer)\n",
    "    X_train_this, X_test_this, y_train_this, y_test_this = \\\n",
    "    train_test_split(Xthis, ythis, test_size=0.2, random_state=42)\n",
    "    clf = MultinomialNB(alpha=alpha).fit(X_train_this, y_train_this)\n",
    "    score = cv_score(clf,X_train_this,y_train_this, f1_scorer)\n",
    "    if score > maxscore_2:\n",
    "        maxscore_2 = score\n",
    "        best_alpha_2 = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"alpha: {}\".format(best_alpha_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.847134\n",
      "Accuracy on test data:     0.750636\n"
     ]
    }
   ],
   "source": [
    "vectorizer_3 = CountVectorizer(min_df = best_min_df_2, tokenizer=dummy_fun, preprocessor=dummy_fun,\n",
    "                              ngram_range=(1,2))\n",
    "X_3, y_3 = make_xy_bigrams(final_posts_df, vectorizer_3)\n",
    "X_train_3, X_test_3, y_train_3, y_test_3= train_test_split(X_3, y_3, test_size=0.2, random_state=42)                                            \n",
    "clf_3 = MultinomialNB(alpha=best_alpha_2).fit(X_train_3, y_train_3)\n",
    "#Print the accuracy on the test and training dataset\n",
    "training_accuracy_3 = clf_3.score(X_train_3, y_train_3)\n",
    "test_accuracy_3 = clf_3.score(X_test_3, y_test_3)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy_3))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onion words\t     P(Onion | word or phrase)\n",
      "               trump 0.96\n",
      "           regularly 0.95\n",
      "             happens 0.94\n",
      "   regularly happens 0.94\n",
      "    nation regularly 0.94\n",
      "         way prevent 0.94\n",
      "          say nation 0.94\n",
      "         prevent say 0.94\n",
      "           announces 0.93\n",
      "                shit 0.93\n",
      "Not Onion words\t     P(Not Onion | word or phrase)\n",
      "                 LOC 0.11\n",
      "             officer 0.11\n",
      "             protest 0.10\n",
      "           Texas GPE 0.10\n",
      "             accused 0.09\n",
      "               China 0.08\n",
      "             ORDINAL 0.06\n",
      "               Texas 0.06\n",
      "            arrested 0.06\n",
      "              police 0.05\n"
     ]
    }
   ],
   "source": [
    "words_3 = np.array(vectorizer_3.get_feature_names())\n",
    "\n",
    "#make identity matrix so that each row has only one word\n",
    "x_3 = np.eye(X_test_3.shape[1])\n",
    "probs_3 = clf_3.predict_log_proba(x_3)[:, 0]\n",
    "ind = np.argsort(probs_3)\n",
    "\n",
    "good_words_3 = words_3[ind[:10]]\n",
    "bad_words_3 = words_3[ind[-10:]]\n",
    "\n",
    "good_prob_3 = probs_3[ind[:10]]\n",
    "bad_prob_3 = probs_3[ind[-10:]]\n",
    "\n",
    "print(\"Onion words\\t     P(Onion | word or phrase)\")\n",
    "for w, p in zip(good_words_3, good_prob_3):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"Not Onion words\\t     P(Not Onion | word or phrase)\")\n",
    "for w, p in zip(bad_words_3, bad_prob_3):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77       200\n",
      "           1       0.78      0.68      0.73       193\n",
      "\n",
      "    accuracy                           0.75       393\n",
      "   macro avg       0.75      0.75      0.75       393\n",
      "weighted avg       0.75      0.75      0.75       393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add code to get classification report and mis-classified sentences\n",
    "pred_3 = clf_3.predict(X_test_3)\n",
    "classification_report_3 = metrics.classification_report(y_test_3,pred_3)\n",
    "print(classification_report_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actually not the Onion but mis-classified as the Onion\n",
      "---------------------------\n",
      "‘Live pee or die’: N.H. governor steps in to let woman keep her ‘PB4WEGO’ license plates\n",
      "\n",
      "White supremacists taking DNA tests sad to discover they’re not 100% white\n",
      "\n",
      "Losers are more likely to believe in conspiracy theories, study finds\n",
      "\n",
      "DeVos backlash Sees Parents Threatening to Homeschool Kids\n",
      "\n",
      "Tobacco smokers could gain 86 million years of life if they switch to vaping, study finds\n",
      "\n",
      "Actually the Onion but mis-classified as not the Onion\n",
      "--------------------------\n",
      "Dallas Cops Plant Black Suspect At Murder Scene\n",
      "\n",
      "‘C’mon, C’mon,’ Says Matt Damon Desperately Searching For Own Name On List Of IMDB User Dolphinsoul60’s Top 100 Actors\n",
      "\n",
      "Blog: If You’re Not A Police Officer, You Can’t Understand The Pressure You Feel In The Split Second When You Have To Decide Whether Or Not To Shoot An Unarmed Civilian 8 Times\n",
      "\n",
      "Heartbroken Russian Ambassador Thought Special Meetings With Jeff Sessions Were Very Memorable\n",
      "\n",
      "Trump Confident U.S. Military Strike On Syria Wiped Out Russian Scandal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_3, y_3 = make_xy_bigrams(final_posts_df, vectorizer_3)\n",
    "\n",
    "prob_3 = clf_3.predict_proba(x_3)[:, 0] #probability of being not the onion\n",
    "predict_3 = clf_3.predict(x_3)\n",
    "\n",
    "y_3 = np.asarray(y_3)\n",
    "misclassified_3 = np.where(y_3 != predict_3) #identify where values don't match prediction\n",
    "\n",
    "series_misclassified_3 = pd.Series(prob_3[misclassified_3], index=list(misclassified_3))\n",
    "\n",
    "#sort series\n",
    "\n",
    "series_misclassified_sorted_3 = series_misclassified_3.sort_values() #get sorted values of misclassified\n",
    "\n",
    "indices_misclassified_3 = list(series_misclassified_sorted_3.index.values) #get indices of misclassified and sort\n",
    "\n",
    "lowest_prob_3 = indices_misclassified_3[0:5]\n",
    "highest_prob_3 = indices_misclassified_3[-5:]\n",
    "\n",
    "lowest_prob_list_3 = [item for t in lowest_prob_3 for item in t] \n",
    "highest_prob_list_3 = [item for t in highest_prob_3 for item in t]\n",
    "\n",
    "###\n",
    "print(\"Actually not the Onion but mis-classified as the Onion\")\n",
    "print('---------------------------')\n",
    "for row in lowest_prob_list_3:\n",
    "    print(all_posts.title.iloc[row])\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Actually the Onion but mis-classified as not the Onion\")\n",
    "print('--------------------------')\n",
    "for row in highest_prob_list_3:\n",
    "    print(all_posts.title.iloc[row])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like bigrams improve the model slightly, so I will use bigrams. I then did analysis on posts from the Onion compared to real news posts from r/news. I will do this using bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles_ents_news = all_posts_news['title'].apply(lambda x: preprocessing_entities(x))\n",
    "final_posts_df_news = pd.DataFrame({'title':all_titles_ents_news, 'Onion?':all_posts_news['Onion?']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the grid of parameters to search over\n",
    "alphas_3 = [.1, 1, 5, 10, 50]\n",
    "best_min_df_3 = 5\n",
    "\n",
    "#Find the best value for alpha and the best classifier\n",
    "best_alpha_3 = None\n",
    "maxscore_3=-np.inf\n",
    "\n",
    "for alpha in alphas_3:   \n",
    "    vectorizer = CountVectorizer(min_df=best_min_df_3, tokenizer=dummy_fun, \\\n",
    "    preprocessor=dummy_fun, ngram_range=(1,2))\n",
    "    Xthis, ythis = make_xy_bigrams(final_posts_df_news, vectorizer)\n",
    "    X_train_this, X_test_this, y_train_this, y_test_this = \\\n",
    "    train_test_split(Xthis, ythis, test_size=0.2, random_state=42)\n",
    "    clf = MultinomialNB(alpha=alpha).fit(X_train_this, y_train_this)\n",
    "    score = cv_score(clf,X_train_this,y_train_this, f1_scorer)\n",
    "    if score > maxscore_3:\n",
    "        maxscore_3 = score\n",
    "        best_alpha_3 = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"alpha: {}\".format(best_alpha_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.877193\n",
      "Accuracy on test data:     0.807500\n"
     ]
    }
   ],
   "source": [
    "vectorizer_4 = CountVectorizer(min_df = best_min_df_3, tokenizer=dummy_fun, preprocessor=dummy_fun,\n",
    "                              ngram_range=(1,2))\n",
    "X_4, y_4 = make_xy_bigrams(final_posts_df_news, vectorizer_4)\n",
    "X_train_4, X_test_4, y_train_4, y_test_4= train_test_split(X_4, y_4, test_size=0.2, random_state=42)                                            \n",
    "clf_4 = MultinomialNB(alpha=best_alpha_3).fit(X_train_4, y_train_4)\n",
    "#Print the accuracy on the test and training dataset\n",
    "training_accuracy_4 = clf_4.score(X_train_4, y_train_4)\n",
    "test_accuracy_4 = clf_4.score(X_test_4, y_test_4)\n",
    "\n",
    "print(\"Accuracy on training data: {:2f}\".format(training_accuracy_4))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onion words\t     P(Onion | word or phrase)\n",
      "                like 1.00\n",
      "             prevent 1.00\n",
      "          study find 0.99\n",
      "             happens 0.99\n",
      "           regularly 0.99\n",
      "         prevent say 0.99\n",
      "             fucking 0.99\n",
      "          say nation 0.99\n",
      "         way prevent 0.99\n",
      "    nation regularly 0.99\n",
      "News words\t     P(News | word or phrase)\n",
      "           GPE MONEY 0.01\n",
      "                 sue 0.01\n",
      "               fired 0.01\n",
      "          California 0.01\n",
      "        CARDINAL GPE 0.01\n",
      "        GPE CARDINAL 0.01\n",
      "              charge 0.01\n",
      "               judge 0.01\n",
      "            arrested 0.01\n",
      "             ORDINAL 0.00\n"
     ]
    }
   ],
   "source": [
    "words_4 = np.array(vectorizer_4.get_feature_names())\n",
    "\n",
    "x_4 = np.eye(X_test_4.shape[1])\n",
    "probs_4 = clf_4.predict_log_proba(x_4)[:, 0]\n",
    "ind = np.argsort(probs_4)\n",
    "\n",
    "good_words_4 = words_4[ind[:10]]\n",
    "bad_words_4 = words_4[ind[-10:]]\n",
    "\n",
    "good_prob_4 = probs_4[ind[:10]]\n",
    "bad_prob_4 = probs_4[ind[-10:]]\n",
    "\n",
    "print(\"Onion words\\t     P(Onion | word or phrase)\")\n",
    "for w, p in zip(good_words_4, good_prob_4):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))\n",
    "    \n",
    "print(\"News words\\t     P(News | word or phrase)\")\n",
    "for w, p in zip(bad_words_4, bad_prob_4):\n",
    "    print(\"{:>20}\".format(w), \"{:.2f}\".format(1 - np.exp(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       206\n",
      "           1       0.83      0.76      0.79       194\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.81      0.81      0.81       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add code to get classification report and mis-classified sentences\n",
    "pred_4 = clf_4.predict(X_test_4)\n",
    "classification_report_4 = metrics.classification_report(y_test_4,pred_4)\n",
    "print(classification_report_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actually news but mis-classified as the Onion\n",
      "---------------------------\n",
      "The Italian government has approved a law ordering parents to vaccinate children or face fines. The authorities have noted a rise in measles cases, which the cabinet blames on \"the spread of anti-scientific theories.\"\n",
      "\n",
      "Decade in the Red: Trump Tax Figures Show Over $1 Billion in Business Losses\n",
      "\n",
      "George Clooney Calls for Online Release of 'The Interview \"That's the most important part. We cannot be told we can't see something by Kim Jong Un, of all f***ing people.\"\n",
      "\n",
      "Facebook \"allowed Microsoft's Bing search engine to see the names of virtually all Facebook users' friends without consent, the records show, and gave Netflix and Spotify the ability to read Facebook users' private messages.\"\n",
      "\n",
      "Millennials earn 20% less than Boomers did at same stage of life\n",
      "\n",
      "Actually the Onion but mis-classified as news\n",
      "--------------------------\n",
      "Man Who Crossed US In Balloon Only Talks About Horse Abuse\n",
      "\n",
      "PR Firm Advises U.S. To Cut Ties With Alabama\n",
      "\n",
      "Roadmap To Peace: Necco Has Set Aside A Roll Of Wafers For Israel And Palestine To Share Only After They Achieve A Two-State Solution\n",
      "\n",
      "‘You Are Donald Trump, 45th President Of The United States,’ Trump Reads From Faded Tattoo On Wrist\n",
      "\n",
      "Trump Confident U.S. Military Strike On Syria Wiped Out Russian Scandal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_4, y_4 = make_xy_bigrams(final_posts_df_news, vectorizer_4)\n",
    "\n",
    "prob_4 = clf_4.predict_proba(x_4)[:, 0] #probability of being news\n",
    "predict_4 = clf_4.predict(x_4)\n",
    "\n",
    "y_4 = np.asarray(y_4)\n",
    "misclassified_4 = np.where(y_4 != predict_4)\n",
    "\n",
    "series_misclassified_4 = pd.Series(prob_4[misclassified_4], index=list(misclassified_4))\n",
    "\n",
    "#sort series\n",
    "\n",
    "series_misclassified_sorted_4 = series_misclassified_4.sort_values() #sort from smallest to largest\n",
    "\n",
    "indices_misclassified_4 = list(series_misclassified_sorted_4.index.values) #get indices of misclassified\n",
    "\n",
    "lowest_prob_4 = indices_misclassified_4[0:5] #get lowest probabilites\n",
    "highest_prob_4 = indices_misclassified_4[-5:] #get highest probabilities\n",
    "\n",
    "lowest_prob_list_4 = [item for t in lowest_prob_4 for item in t]  #get \n",
    "highest_prob_list_4 = [item for t in highest_prob_4 for item in t]\n",
    "\n",
    "###\n",
    "print(\"Actually news but mis-classified as the Onion\")\n",
    "print('---------------------------')\n",
    "for row in lowest_prob_list_4:\n",
    "    print(all_posts_news.title.iloc[row])\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Actually the Onion but mis-classified as news\")\n",
    "print('--------------------------')\n",
    "for row in highest_prob_list_4:\n",
    "    print(all_posts_news.title.iloc[row])\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
